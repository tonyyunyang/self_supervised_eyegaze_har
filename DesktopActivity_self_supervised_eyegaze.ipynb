{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ensure reproducibility\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "print(os.environ.get(\"CUBLAS_WORKSPACE_CONFIG\")) # Default is None\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "print(os.environ.get(\"CUBLAS_WORKSPACE_CONFIG\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import torch\n",
    "import h5py\n",
    "\n",
    "from torch import optim\n",
    "from pprint import pprint\n",
    "from utils.plant_seed import plant_seed\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.optimizer import CosineScheduler\n",
    "from utils.dataset import SelfSupervisedDataset, FullySupervisedDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.dataset import extract_parameters_from_datatype, check_indices_overlap\n",
    "from utils.load_model import load_create_imputation_model, load_create_classification_model, load_create_original_imputation_model, load_create_original_classification_model\n",
    "from utils.load_data import get_self_supervised_pretrain_indices, get_fully_supervised_finetune_indices, safe_train_test_split\n",
    "from modules.loss import MaskedMSELoss\n",
    "from utils.train_model import train_self_supervised_pretrain_model, eval_best_imputation_model, eval_last_imputation_model, train_self_supervised_finetune_model, eval_last_model, eval_best_model, train_self_supervised_finetune_model_no_val\n",
    "\n",
    "\n",
    "seed = 0\n",
    "seed_worker, g = plant_seed(seed)"
   ],
   "id": "73ff7a0f8a53b219",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "label_map = {\n",
    "    0: \"BROWSE\",\n",
    "    1: \"PLAY\",\n",
    "    2: \"READ\",\n",
    "    3: \"SEARCH\",\n",
    "    4: \"WATCH\",\n",
    "    5: \"WRITE\"\n",
    "}"
   ],
   "id": "cc8f35b133b6a22e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config_file_path = \"utils/DesktopActivity_config.json\"\n",
    "with open(config_file_path, \"r\") as file:\n",
    "    config = json.load(file)\n",
    "pprint(config)"
   ],
   "id": "eb1dd6fd7f4e077",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "overlap, window_seconds, window_length = extract_parameters_from_datatype(config['data_type'])\n",
    "print(f\"overlap {overlap}, window seconds: {window_seconds}, window length: {window_length}\")\n",
    "\n",
    "# assign the window length to the config\n",
    "config['kdd_model']['max_seq_len'] = window_length\n",
    "config['kdd_original_model']['max_seq_len'] = window_length"
   ],
   "id": "72cf9527f6db50b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "subjects_dict_path = os.path.join(config['data_path'], config['data_type'], 'starting_indices.json')\n",
    "with open(subjects_dict_path, \"r\") as file:\n",
    "    subjects = json.load(file)\n",
    "pprint(subjects)\n",
    "\n",
    "data_file_path = os.path.join(config['data_path'], config['data_type'], f\"{config['data_type']}.h5\")\n",
    "with h5py.File(data_file_path, 'r') as h5_file:\n",
    "    last_index = h5_file['training_data'].shape[0] - 1\n",
    "    # print(last_index)"
   ],
   "id": "953c4affa3db1675",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_acc = 0\n",
    "total_f1 = 0\n",
    "# Pretrain Loop\n",
    "for leave_out_subject in subjects:\n",
    "    print(f\"Leave out subject: {leave_out_subject}\")\n",
    "    pretrain_test_indices, pretrain_train_indices = get_self_supervised_pretrain_indices(subjects, leave_out_subject, last_index)\n",
    "    \n",
    "    # Print the sizes of each split for pretraining\n",
    "    print(f\"Pretraining data split:\")\n",
    "    print(f\"  Train set size: {len(pretrain_train_indices)}\")\n",
    "    print(f\"  Test set size: {len(pretrain_test_indices)}\")\n",
    "\n",
    "    train_dataset = SelfSupervisedDataset(data_file_path, pretrain_train_indices, mean_mask_length=3, masking_ratio=0.10)\n",
    "    test_dataset = SelfSupervisedDataset(data_file_path, pretrain_test_indices, mean_mask_length=3, masking_ratio=0.10)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['pretrain_batch_size'], shuffle=True, num_workers=0, worker_init_fn=seed_worker, generator=g) \n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['pretrain_batch_size'], shuffle=False, num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "    loaders = (train_loader, test_loader)\n",
    "\n",
    "    model, model_config = load_create_original_imputation_model(config)\n",
    "\n",
    "    criterion = MaskedMSELoss()\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=1.0, betas=(0.9, 0.999))\n",
    "    optimizer = optim.RAdam(model.parameters(), lr=1.0, betas=(0.9, 0.999))\n",
    "    scheduler = CosineScheduler(max_update=config['pretrain_max_update_epochs'], base_lr=config['pretrain_base_lr'], final_lr=config['pretrain_final_lr'], warmup_steps=config['pretrain_warmup_epochs'], warmup_begin_lr=0.0)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler)\n",
    "\n",
    "\n",
    "    tensorboard = train_self_supervised_pretrain_model(model, criterion, optimizer, scheduler, loaders, model_config, config, leave_out_subject)\n",
    "\n",
    "    eval_last_imputation_model(model, test_loader, config, os.path.join(config['model_path'], \"imputate_result\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ========================================================================================================================\n",
    "    # Finetune the model\n",
    "    config['pretrain_model_path'] = config['model_path']\n",
    "    # config['model_path'] is where the model is saved\n",
    "    finetune_test_indices, finetune_train_indices = get_fully_supervised_finetune_indices(pretrain_test_indices, data_file_path, finetune_proportion=config['finetune_proportion'])\n",
    "    \n",
    "    if config['finetune_train_proportion']:\n",
    "        available_pretrain_train_indices, _ = safe_train_test_split(pretrain_train_indices, train_size=config['pretrain_label_availability'], random_state=seed)\n",
    "        \n",
    "        all_train_indices = np.concatenate([finetune_train_indices, available_pretrain_train_indices])\n",
    "        finetune_train_indices, finetune_val_indices = train_test_split(all_train_indices, train_size=config['finetune_train_proportion'], random_state=seed)\n",
    "        finetune_test_indices = np.array(finetune_test_indices)\n",
    "        \n",
    "        # Print the sizes of each split for finetuning\n",
    "        print(f\"Finetuning data split:\")\n",
    "        print(f\"  Train set size: {len(finetune_train_indices)}\")\n",
    "        print(f\"  Validation set size: {len(finetune_val_indices)}\")\n",
    "        print(f\"  Test set size: {len(finetune_test_indices)}\")\n",
    "    \n",
    "        train_dataset = FullySupervisedDataset(data_file_path, finetune_train_indices, label_map)\n",
    "        val_dataset = FullySupervisedDataset(data_file_path, finetune_val_indices, label_map)\n",
    "        test_dataset = FullySupervisedDataset(data_file_path, finetune_test_indices, label_map)\n",
    "    \n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['finetune_batch_size'], shuffle=True, num_workers=0, worker_init_fn=seed_worker, generator=g) \n",
    "        val_loader = DataLoader(val_dataset, batch_size=config['finetune_batch_size'], shuffle=False, num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=config['finetune_batch_size'], shuffle=False, num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "    \n",
    "        loaders = (train_loader, val_loader, test_loader)\n",
    "    else:\n",
    "        available_pretrain_train_indices, _ = safe_train_test_split(pretrain_train_indices, train_size=config['pretrain_label_availability'], random_state=seed)\n",
    "        \n",
    "        finetune_train_indices = np.concatenate([finetune_train_indices, available_pretrain_train_indices])\n",
    "        finetune_test_indices = np.array(finetune_test_indices)\n",
    "        \n",
    "        # Print the sizes of each split for finetuning without validation\n",
    "        print(f\"Finetuning data split (no validation):\")\n",
    "        print(f\"  Train set size: {len(finetune_train_indices)}\")\n",
    "        print(f\"  Test set size: {len(finetune_test_indices)}\")\n",
    "        \n",
    "        # sys.exit(\"Not yet done!\")\n",
    "        train_dataset = FullySupervisedDataset(data_file_path, finetune_train_indices, label_map)\n",
    "        test_dataset = FullySupervisedDataset(data_file_path, finetune_test_indices, label_map)\n",
    "    \n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['finetune_batch_size'], shuffle=True, num_workers=0, worker_init_fn=seed_worker, generator=g) \n",
    "        test_loader = DataLoader(test_dataset, batch_size=config['finetune_batch_size'], shuffle=False, num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "        \n",
    "        loaders = (train_loader, test_loader)\n",
    "\n",
    "    model, model_config = load_create_original_classification_model(config, num_classes=len(label_map))\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=config['label_smoothing'])\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=1.0, betas=(0.9, 0.999))\n",
    "    optimizer = optim.RAdam(model.parameters(), lr=1.0, betas=(0.9, 0.999))\n",
    "    scheduler = CosineScheduler(max_update=config['finetune_max_update_epochs'], base_lr=config['finetune_base_lr'], final_lr=config['finetune_final_lr'], warmup_steps=config['finetune_warmup_epochs'], warmup_begin_lr=0.0)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler)\n",
    "    \n",
    "    if config['finetune_train_proportion']:\n",
    "        tensorboard_writer = train_self_supervised_finetune_model(model, criterion, optimizer, scheduler, loaders, model_config, config)\n",
    "        \n",
    "        best_model_acc, best_model_f1 = eval_best_model(model, test_loader, config, label_map)\n",
    "        last_model_acc, last_model_f1 = eval_last_model(model, test_loader, config, label_map)\n",
    "        \n",
    "        test_acc = max(best_model_acc, last_model_acc)\n",
    "        test_f1 = max(best_model_f1, last_model_f1)\n",
    "    else:\n",
    "        # sys.exit(\"Not yet done!\")\n",
    "        tensorboard_writer = train_self_supervised_finetune_model_no_val(model, criterion, optimizer, scheduler, loaders, model_config, config)\n",
    "        \n",
    "        last_model_acc, last_model_f1 = eval_last_model(model, test_loader, config, label_map)\n",
    "        \n",
    "        test_acc = last_model_acc\n",
    "        test_f1 = last_model_f1\n",
    "    \n",
    "    # Accumulate accuracy and F1 score\n",
    "    total_acc += test_acc\n",
    "    total_f1 += test_f1\n",
    "    \n",
    "    # clear the config['pretrain_model_path'] to None for next subject\n",
    "    config['pretrain_model_path'] = None"
   ],
   "id": "9f06ff4baf389c8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute the average accuracy and F1 score after the loop\n",
    "average_acc = total_acc / len(subjects)\n",
    "average_f1 = total_f1 / len(subjects)\n",
    "\n",
    "print(f\"Average Accuracy across all folds: {average_acc}\")\n",
    "print(f\"Average F1 Score across all folds: {average_f1}\")\n",
    "\n",
    "# Get the current directory from the config\n",
    "current_dir = config['model_path']\n",
    "\n",
    "# Define the filename for the average scores\n",
    "filename = \"average_acc_and_f1.txt\"\n",
    "\n",
    "# Full path for the file\n",
    "file_path = os.path.join(current_dir, filename)\n",
    "\n",
    "# Write the average scores to the file\n",
    "with open(file_path, 'w') as f:\n",
    "    f.write(f\"Average Accuracy: {average_acc}\\n\")\n",
    "    f.write(f\"Average F1 Score: {average_f1}\\n\")\n",
    "\n",
    "print(f\"Saved average scores to {file_path}\")"
   ],
   "id": "d5adaf5f492fc6b0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
