{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ensure reproducibility\n",
    "import os\n",
    "print(os.environ.get(\"CUBLAS_WORKSPACE_CONFIG\")) # Default is None\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "print(os.environ.get(\"CUBLAS_WORKSPACE_CONFIG\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import torch\n",
    "import h5py\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch import optim\n",
    "from pprint import pprint\n",
    "from utils.plant_seed import plant_seed\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.optimizer import CosineScheduler\n",
    "from utils.dataset import FullySupervisedDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.dataset import extract_parameters_from_datatype, check_indices_overlap\n",
    "from utils.load_model import load_create_classification_model\n",
    "from utils.load_data import get_fully_supervised_pretrain_indices\n",
    "from utils.train_model import train_fully_supervised_pretrain_model, eval_best_model, eval_last_model\n",
    "\n",
    "\n",
    "seed = 0\n",
    "seed_worker, g = plant_seed(seed)"
   ],
   "id": "73ff7a0f8a53b219",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "label_map = {\n",
    "    0: \"BROWSE\",\n",
    "    1: \"PLAY\",\n",
    "    2: \"READ\",\n",
    "    3: \"SEARCH\",\n",
    "    4: \"WATCH\",\n",
    "    5: \"WRITE\"\n",
    "}"
   ],
   "id": "cc8f35b133b6a22e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config_file_path = \"utils/DesktopActivity_config.json\"\n",
    "with open(config_file_path, \"r\") as file:\n",
    "    config = json.load(file)\n",
    "pprint(config)"
   ],
   "id": "eb1dd6fd7f4e077",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "overlap, window_seconds, window_length = extract_parameters_from_datatype(config['data_type'])\n",
    "print(f\"overlap {overlap}, window seconds: {window_seconds}, window length: {window_length}\")\n",
    "\n",
    "# assign the window length to the config\n",
    "config['kdd_model']['max_seq_len'] = window_length"
   ],
   "id": "72cf9527f6db50b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "subjects_dict_path = os.path.join(config['data_path'], config['data_type'], 'starting_indices.json')\n",
    "with open(subjects_dict_path, \"r\") as file:\n",
    "    subjects = json.load(file)\n",
    "pprint(subjects)\n",
    "\n",
    "data_file_path = os.path.join(config['data_path'], config['data_type'], f\"{config['data_type']}.h5\")\n",
    "with h5py.File(data_file_path, 'r') as h5_file:\n",
    "    last_index = h5_file['training_data'].shape[0] - 1\n",
    "    # print(last_index)"
   ],
   "id": "953c4affa3db1675",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pretrain Loop\n",
    "total_acc = 0\n",
    "total_f1 = 0\n",
    "for leave_out_subject in subjects:\n",
    "    print(f\"Leave out subject: {leave_out_subject}\")\n",
    "    pretrain_test_indices, pretrain_train_indices = get_fully_supervised_pretrain_indices(subjects, leave_out_subject, last_index)\n",
    "    # split train indices into train and validation\n",
    "    pretrain_train_indices, pretrain_val_indices = train_test_split(pretrain_train_indices, test_size=config['pretrain_proportion'], random_state=seed)\n",
    "    \n",
    "    overlap_check = check_indices_overlap(pretrain_train_indices, pretrain_val_indices, pretrain_test_indices)\n",
    "    if not overlap_check:\n",
    "        print(\"Warning: Overlap detected between datasets. Please review the index splitting process.\")\n",
    "    else:\n",
    "        print(\"No overlap detected between datasets.\")\n",
    "    \n",
    "    train_dataset = FullySupervisedDataset(data_file_path, pretrain_train_indices, label_map)\n",
    "    val_dataset = FullySupervisedDataset(data_file_path, pretrain_val_indices, label_map)\n",
    "    test_dataset = FullySupervisedDataset(data_file_path, pretrain_test_indices, label_map)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['pretrain_batch_size'], shuffle=True, num_workers=0, worker_init_fn=seed_worker, generator=g) \n",
    "    val_loader = DataLoader(train_dataset, batch_size=config['pretrain_batch_size'], shuffle=False, num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['pretrain_batch_size'], shuffle=False, num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "    \n",
    "    loaders = (train_loader, val_loader, test_loader)\n",
    "    \n",
    "    model, model_config = load_create_classification_model(config, num_classes=len(label_map))\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=config['label_smoothing'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1.0, betas=(0.9, 0.999))\n",
    "    scheduler = CosineScheduler(max_update=config['pretrain_max_update_epochs'], base_lr=config['pretrain_base_lr'], final_lr=config['pretrain_final_lr'], warmup_steps=config['pretrain_warmup_epochs'], warmup_begin_lr=0.0)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler)\n",
    "    \n",
    "    tensorboard_writer = train_fully_supervised_pretrain_model(model, criterion, optimizer, scheduler, loaders, model_config, config, leave_out_subject)\n",
    "    \n",
    "    best_model_acc, best_model_f1 = eval_best_model(model, test_loader, config, label_map)\n",
    "    \n",
    "    last_model_acc, last_model_f1 = eval_last_model(model, test_loader, config, label_map)\n",
    "    \n",
    "    test_acc = max(best_model_acc, last_model_acc)\n",
    "    test_f1 = max(best_model_f1, last_model_f1)\n",
    "    \n",
    "    # Accumulate accuracy and F1 score\n",
    "    total_acc += test_acc\n",
    "    total_f1 += test_f1"
   ],
   "id": "9f06ff4baf389c8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute the average accuracy and F1 score after the loop\n",
    "average_acc = total_acc / len(subjects)\n",
    "average_f1 = total_f1 / len(subjects)\n",
    "\n",
    "print(f\"Average Accuracy across all folds: {average_acc}\")\n",
    "print(f\"Average F1 Score across all folds: {average_f1}\")\n",
    "\n",
    "# Get the current directory from the config\n",
    "current_dir = config['model_path']\n",
    "\n",
    "# Navigate to the parent directory of current_dir\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Define the filename for the average scores\n",
    "filename = \"average_acc_and_f1.txt\"\n",
    "\n",
    "# Full path for the file\n",
    "file_path = os.path.join(parent_dir, filename)\n",
    "\n",
    "# Write the average scores to the file\n",
    "with open(file_path, 'w') as f:\n",
    "    f.write(f\"Average Accuracy: {average_acc}\\n\")\n",
    "    f.write(f\"Average F1 Score: {average_f1}\\n\")\n",
    "\n",
    "print(f\"Saved average scores to {file_path}\")"
   ],
   "id": "d1e0d374b29d74e0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
