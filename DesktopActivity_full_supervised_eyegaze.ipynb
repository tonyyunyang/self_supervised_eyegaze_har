{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-02T22:51:00.205746Z",
     "start_time": "2024-08-02T22:50:59.433043Z"
    }
   },
   "source": [
    "# Ensure reproducibility\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "print(os.environ.get(\"CUBLAS_WORKSPACE_CONFIG\")) # Default is None\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "print(os.environ.get(\"CUBLAS_WORKSPACE_CONFIG\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      ":4096:8\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T22:51:01.090257Z",
     "start_time": "2024-08-02T22:51:00.206847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import torch\n",
    "import h5py\n",
    "\n",
    "from torch import optim\n",
    "from pprint import pprint\n",
    "from utils.plant_seed import plant_seed\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.optimizer import CosineScheduler\n",
    "from utils.dataset import FullySupervisedDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.dataset import extract_parameters_from_datatype, check_indices_overlap\n",
    "from utils.load_model import load_create_classification_model, load_create_original_classification_model\n",
    "from utils.load_data import split_leave_out_rest_sub_sample_indices, get_fully_supervised_finetune_indices, safe_train_test_split\n",
    "from utils.train_model import train_fully_supervised_model, eval_best_model, eval_last_model, train_fully_supervised_model_no_val\n",
    "\n",
    "\n",
    "seed = 0\n",
    "seed_worker, g = plant_seed(seed)"
   ],
   "id": "73ff7a0f8a53b219",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T22:51:01.093088Z",
     "start_time": "2024-08-02T22:51:01.091079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label_map = {\n",
    "    0: \"BROWSE\",\n",
    "    1: \"PLAY\",\n",
    "    2: \"READ\",\n",
    "    3: \"SEARCH\",\n",
    "    4: \"WATCH\",\n",
    "    5: \"WRITE\"\n",
    "}"
   ],
   "id": "cc8f35b133b6a22e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T22:51:01.105752Z",
     "start_time": "2024-08-02T22:51:01.093815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config_file_path = \"utils/DesktopActivity_config.json\"\n",
    "with open(config_file_path, \"r\") as file:\n",
    "    config = json.load(file)\n",
    "pprint(config)"
   ],
   "id": "eb1dd6fd7f4e077",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'V1Conv_5sec': {'dilation': 1, 'kernel_size': 20, 'padding': 0, 'stride': 10},\n",
      " 'V2Conv_10sec': {'dilation': 1, 'kernel_size': 30, 'padding': 0, 'stride': 15},\n",
      " 'V3Conv_15sec': {'dilation': 1, 'kernel_size': 40, 'padding': 0, 'stride': 10},\n",
      " 'V4Conv_20sec': {'dilation': 1, 'kernel_size': 30, 'padding': 0, 'stride': 15},\n",
      " 'V5Conv_25sec': {'dilation': 1, 'kernel_size': 40, 'padding': 0, 'stride': 20},\n",
      " 'V6Conv_30sec': {'dilation': 1, 'kernel_size': 30, 'padding': 0, 'stride': 15},\n",
      " 'data_path': 'dataset/training_data/DesktopActivity_std_norm',\n",
      " 'data_type': 'overlap_0.8_window_15s',\n",
      " 'downstream_proportion': 0.1,\n",
      " 'downstream_training_proportion': None,\n",
      " 'finetune_base_lr': 0.0001,\n",
      " 'finetune_batch_size': 64,\n",
      " 'finetune_epoch': 11,\n",
      " 'finetune_final_lr': 0.0001,\n",
      " 'finetune_max_update_epochs': 11,\n",
      " 'finetune_proportion': 0.1,\n",
      " 'finetune_train_proportion': 0.8,\n",
      " 'finetune_warmup_epochs': 10,\n",
      " 'fully_supervised_base_lr': 0.0001,\n",
      " 'fully_supervised_batch_size': 64,\n",
      " 'fully_supervised_epoch': 11,\n",
      " 'fully_supervised_final_lr': 0.0001,\n",
      " 'fully_supervised_max_update_epochs': 11,\n",
      " 'fully_supervised_warmup_epochs': 10,\n",
      " 'kdd_model': {'conv_config': 'V3Conv_15sec',\n",
      "               'd_model': 64,\n",
      "               'dim_feedforward': 256,\n",
      "               'emb_dropout': 0.1,\n",
      "               'embedding': 'convolution',\n",
      "               'enc_dropout': 0.1,\n",
      "               'feat_dim': 2,\n",
      "               'max_seq_len': None,\n",
      "               'n_heads': 8,\n",
      "               'n_layers': 3},\n",
      " 'kdd_original_model': {'conv_config': 'V3Conv_15sec',\n",
      "                        'd_model': 64,\n",
      "                        'dim_feedforward': 256,\n",
      "                        'emb_dropout': 0.1,\n",
      "                        'embedding': 'convolution',\n",
      "                        'enc_dropout': 0.1,\n",
      "                        'feat_dim': 2,\n",
      "                        'max_seq_len': None,\n",
      "                        'n_heads': 8,\n",
      "                        'n_layers': 3,\n",
      "                        'pre_norm': False},\n",
      " 'label_smoothing': 0.0,\n",
      " 'pretrain_base_lr': 0.001,\n",
      " 'pretrain_batch_size': 64,\n",
      " 'pretrain_epoch': 11,\n",
      " 'pretrain_final_lr': 0.001,\n",
      " 'pretrain_label_availability': 1.0,\n",
      " 'pretrain_max_update_epochs': 11,\n",
      " 'pretrain_model_path': None,\n",
      " 'pretrain_warmup_epochs': 10,\n",
      " 'result_path': 'results/DesktopActivity',\n",
      " 'upstream_label_availability': 1.0}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T22:51:01.110777Z",
     "start_time": "2024-08-02T22:51:01.106581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "overlap, window_seconds, window_length = extract_parameters_from_datatype(config['data_type'])\n",
    "print(f\"overlap {overlap}, window seconds: {window_seconds}, window length: {window_length}\")\n",
    "\n",
    "# assign the window length to the config\n",
    "config['kdd_model']['max_seq_len'] = window_length\n",
    "config['kdd_original_model']['max_seq_len'] = window_length"
   ],
   "id": "72cf9527f6db50b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap 0.8, window seconds: 15, window length: 450\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T22:51:01.116629Z",
     "start_time": "2024-08-02T22:51:01.111424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subjects_dict_path = os.path.join(config['data_path'], config['data_type'], 'starting_indices.json')\n",
    "with open(subjects_dict_path, \"r\") as file:\n",
    "    subjects = json.load(file)\n",
    "pprint(subjects)\n",
    "\n",
    "data_file_path = os.path.join(config['data_path'], config['data_type'], f\"{config['data_type']}.h5\")\n",
    "with h5py.File(data_file_path, 'r') as h5_file:\n",
    "    last_index = h5_file['training_data'].shape[0] - 1\n",
    "    # print(last_index)"
   ],
   "id": "953c4affa3db1675",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'P01': 0,\n",
      " 'P02': 582,\n",
      " 'P03': 1164,\n",
      " 'P04': 1746,\n",
      " 'P05': 2328,\n",
      " 'P06': 2910,\n",
      " 'P07': 3492,\n",
      " 'P08': 4074}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T22:51:09.739409Z",
     "start_time": "2024-08-02T22:51:01.117312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training Loop\n",
    "total_acc = 0\n",
    "total_f1 = 0\n",
    "for leave_out_subject in subjects:\n",
    "    print(f\"Leave out subject: {leave_out_subject}\")\n",
    "    \n",
    "    leave_out_sub_sample_indices, rest_sub_sample_indices = split_leave_out_rest_sub_sample_indices(subjects, leave_out_subject, last_index)\n",
    "        \n",
    "    leave_out_sub_test_indices, leave_out_sub_train_indices = get_fully_supervised_finetune_indices(leave_out_sub_sample_indices, data_file_path, finetune_proportion=config['downstream_proportion'])\n",
    "    \n",
    "    if config['downstream_training_proportion']:\n",
    "        available_rest_sub_train_indices, _ = safe_train_test_split(rest_sub_sample_indices, train_size=config['upstream_label_availability'], random_state=seed)\n",
    "        \n",
    "        all_train_indices = np.concatenate([leave_out_sub_train_indices, available_rest_sub_train_indices])\n",
    "        \n",
    "        train_indices, val_indices = train_test_split(all_train_indices, train_size=config['downstream_training_proportion'], random_state=seed)\n",
    "        \n",
    "        test_indices = leave_out_sub_test_indices\n",
    "        \n",
    "        print(f\"Train indices: {train_indices.shape[0]}, Val indices: {val_indices.shape[0]}, Test indices: {test_indices.shape[0]}\")\n",
    "        \n",
    "        train_dataset = FullySupervisedDataset(data_file_path, train_indices, label_map)\n",
    "        val_dataset = FullySupervisedDataset(data_file_path, val_indices, label_map)\n",
    "        test_dataset = FullySupervisedDataset(data_file_path, test_indices, label_map)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['fully_supervised_batch_size'], shuffle=True, num_workers=0, worker_init_fn=seed_worker, generator=g) \n",
    "        val_loader = DataLoader(val_dataset, batch_size=config['fully_supervised_batch_size'], shuffle=False, num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=config['fully_supervised_batch_size'], shuffle=False, num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "        \n",
    "        loaders = (train_loader, val_loader, test_loader)\n",
    "        \n",
    "    else:\n",
    "        available_rest_sub_train_indices, _ = safe_train_test_split(rest_sub_sample_indices, train_size=config['upstream_label_availability'], random_state=seed)\n",
    "        \n",
    "        train_indices = np.concatenate([leave_out_sub_train_indices, available_rest_sub_train_indices])\n",
    "        \n",
    "        test_indices = np.array(leave_out_sub_test_indices)\n",
    "        \n",
    "        print(f\"Train indices: {train_indices.shape[0]}, Test indices: {test_indices.shape[0]}\")\n",
    "        \n",
    "        train_dataset = FullySupervisedDataset(data_file_path, train_indices, label_map)\n",
    "        test_dataset = FullySupervisedDataset(data_file_path, test_indices, label_map)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['fully_supervised_batch_size'], shuffle=True, num_workers=0, worker_init_fn=seed_worker, generator=g) \n",
    "        test_loader = DataLoader(test_dataset, batch_size=config['fully_supervised_batch_size'], shuffle=False, num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "    \n",
    "        loaders = (train_loader, test_loader)\n",
    "        \n",
    "    model, model_config = load_create_original_classification_model(config, num_classes=len(label_map))\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=config['label_smoothing'])\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=1.0, betas=(0.9, 0.999))\n",
    "    optimizer = optim.RAdam(model.parameters(), lr=1.0, betas=(0.9, 0.999))\n",
    "    scheduler = CosineScheduler(max_update=config['fully_supervised_max_update_epochs'], base_lr=config['fully_supervised_base_lr'], final_lr=config['fully_supervised_final_lr'], warmup_steps=config['fully_supervised_warmup_epochs'], warmup_begin_lr=0.0)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler)\n",
    "    \n",
    "    if config['downstream_training_proportion']:\n",
    "        tensorboard_writer = train_fully_supervised_model(model, criterion, optimizer, scheduler, loaders, model_config, config, leave_out_subject)\n",
    "    \n",
    "        best_model_acc, best_model_f1 = eval_best_model(model, test_loader, config, label_map)\n",
    "        \n",
    "        last_model_acc, last_model_f1 = eval_last_model(model, test_loader, config, label_map)\n",
    "        \n",
    "        test_acc = max(best_model_acc, last_model_acc)\n",
    "        test_f1 = max(best_model_f1, last_model_f1)\n",
    "        \n",
    "    else:\n",
    "        tensorboard_writer = train_fully_supervised_model_no_val(model, criterion, optimizer, scheduler, loaders, model_config, config, leave_out_subject)\n",
    "        \n",
    "        last_model_acc, last_model_f1 = eval_last_model(model, test_loader, config, label_map)\n",
    "        \n",
    "        test_acc = last_model_acc\n",
    "        test_f1 = last_model_f1\n",
    "\n",
    "    # Accumulate accuracy and F1 score\n",
    "    total_acc += test_acc\n",
    "    total_f1 += test_f1"
   ],
   "id": "9f06ff4baf389c8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leave out subject: P01\n",
      "Distribution of indices across labels for train set:\n",
      "  Label b'BROWSE': 9 indices\n",
      "  Label b'PLAY': 9 indices\n",
      "  Label b'READ': 9 indices\n",
      "  Label b'SEARCH': 9 indices\n",
      "  Label b'WATCH': 9 indices\n",
      "  Label b'WRITE': 9 indices\n",
      "Total train indices: 54\n",
      "Total test indices: 528\n",
      "Train indices: 4128, Test indices: 528\n",
      "{'conv_config': {'dilation': 1, 'kernel_size': 40, 'padding': 0, 'stride': 10},\n",
      " 'd_model': 64,\n",
      " 'dim_feedforward': 256,\n",
      " 'emb_dropout': 0.1,\n",
      " 'embedding': 'convolution',\n",
      " 'enc_dropout': 0.1,\n",
      " 'feat_dim': 2,\n",
      " 'max_len': 450,\n",
      " 'n_heads': 8,\n",
      " 'n_layers': 3,\n",
      " 'num_classes': 6,\n",
      " 'pre_norm': False}\n",
      "Convolutional embedding: 42 sequence length.\n",
      "=============================================================\n",
      "=====================Training via cuda===================\n",
      "=============================================================\n",
      "Run cmd: tensorboard --logdir=results/DesktopActivity/overlap_0.8_window_15s/fully_supervised_downstream_0.1_upstream_avail_1.0/feat_dim_2_d_model_64_n_heads_8_n_layers_3_d_ff_256_emb_dropout_0.1_enc_dropout_0.1_embedding_convolution_conv_config_V3Conv_15sec/epochs_11_max_update_steps_11_warmup_steps_10_batch_size_64_base_lr_0.0001_final_lr_0.0001_label_smoothing_0/test_sub_P01/TensorBoard_Log then open http://localhost:6006\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 67\u001B[0m\n\u001B[1;32m     64\u001B[0m     test_f1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(best_model_f1, last_model_f1)\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 67\u001B[0m     tensorboard_writer \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_fully_supervised_model_no_val\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mleave_out_subject\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     69\u001B[0m     last_model_acc, last_model_f1 \u001B[38;5;241m=\u001B[39m eval_last_model(model, test_loader, config, label_map)\n\u001B[1;32m     71\u001B[0m     test_acc \u001B[38;5;241m=\u001B[39m last_model_acc\n",
      "File \u001B[0;32m~/Desktop/new_summer_research/utils/train_model.py:172\u001B[0m, in \u001B[0;36mtrain_fully_supervised_model_no_val\u001B[0;34m(model, loss, optimizer, scheduler, loaders, model_config, config, leave_out_subject)\u001B[0m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfully_supervised_epoch\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m    170\u001B[0m     writer\u001B[38;5;241m.\u001B[39madd_scalar(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLearning_Rate\u001B[39m\u001B[38;5;124m'\u001B[39m, optimizer\u001B[38;5;241m.\u001B[39mparam_groups[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m'\u001B[39m], epoch)\n\u001B[0;32m--> 172\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mpass_epoch_no_val\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    174\u001B[0m     train_loss_list\u001B[38;5;241m.\u001B[39mappend(train_loss)\n\u001B[1;32m    176\u001B[0m     \u001B[38;5;66;03m# test_acc, test_f1 = force_eval_model(model, test_set, device)\u001B[39;00m\n\u001B[1;32m    177\u001B[0m     \u001B[38;5;66;03m# writer.add_scalar('Force_Test/Accuracy', test_acc, epoch)\u001B[39;00m\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;66;03m# writer.add_scalar('Force_Test/F1_Score', test_f1, epoch)\u001B[39;00m\n\u001B[1;32m    179\u001B[0m \n\u001B[1;32m    180\u001B[0m     \u001B[38;5;66;03m# Log training and validation metrics\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/new_summer_research/utils/train_model.py:422\u001B[0m, in \u001B[0;36mpass_epoch_no_val\u001B[0;34m(model, loss, optimizer, scheduler, train_set, device)\u001B[0m\n\u001B[1;32m    419\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    420\u001B[0m total_train_samples \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m--> 422\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain_set\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    423\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmvts_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\n\u001B[1;32m    424\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmvts_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmvts_inputs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    672\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    673\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 674\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    675\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    676\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Desktop/new_summer_research/utils/dataset.py:43\u001B[0m, in \u001B[0;36mFullySupervisedDataset.__getitem__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m     40\u001B[0m index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[item]\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# Load the data from the file\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m data \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfile\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtraining_data\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m[index])\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[1;32m     45\u001B[0m label_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfile[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m][index][\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     47\u001B[0m label_int \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_map[label_str])\n",
      "File \u001B[0;32mh5py/_objects.pyx:54\u001B[0m, in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mh5py/_objects.pyx:55\u001B[0m, in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/h5py/_hl/group.py:366\u001B[0m, in \u001B[0;36mGroup.__getitem__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    364\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Group(oid)\n\u001B[1;32m    365\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m otype \u001B[38;5;241m==\u001B[39m h5i\u001B[38;5;241m.\u001B[39mDATASET:\n\u001B[0;32m--> 366\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m dataset\u001B[38;5;241m.\u001B[39mDataset(oid, readonly\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m    367\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m otype \u001B[38;5;241m==\u001B[39m h5i\u001B[38;5;241m.\u001B[39mDATATYPE:\n\u001B[1;32m    368\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m datatype\u001B[38;5;241m.\u001B[39mDatatype(oid)\n",
      "File \u001B[0;32mh5py/_objects.pyx:54\u001B[0m, in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mh5py/_objects.pyx:55\u001B[0m, in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/h5py/_hl/files.py:316\u001B[0m, in \u001B[0;36mFile.mode\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    313\u001B[0m         drivers[h5fd\u001B[38;5;241m.\u001B[39mDIRECT] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdirect\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    314\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m drivers\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mid\u001B[38;5;241m.\u001B[39mget_access_plist()\u001B[38;5;241m.\u001B[39mget_driver(), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124munknown\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 316\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;129m@with_phil\u001B[39m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmode\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    319\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" Python mode used to open file \"\"\"\u001B[39;00m\n\u001B[1;32m    320\u001B[0m     write_intent \u001B[38;5;241m=\u001B[39m h5f\u001B[38;5;241m.\u001B[39mACC_RDWR\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute the average accuracy and F1 score after the loop\n",
    "average_acc = total_acc / len(subjects)\n",
    "average_f1 = total_f1 / len(subjects)\n",
    "\n",
    "print(f\"Average Accuracy across all folds: {average_acc}\")\n",
    "print(f\"Average F1 Score across all folds: {average_f1}\")\n",
    "\n",
    "# Get the current directory from the config\n",
    "current_dir = config['model_path']\n",
    "\n",
    "# Navigate to the parent directory of current_dir\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Define the filename for the average scores\n",
    "filename = \"average_acc_and_f1.txt\"\n",
    "\n",
    "# Full path for the file\n",
    "file_path = os.path.join(parent_dir, filename)\n",
    "\n",
    "# Write the average scores to the file\n",
    "with open(file_path, 'w') as f:\n",
    "    f.write(f\"Average Accuracy: {average_acc}\\n\")\n",
    "    f.write(f\"Average F1 Score: {average_f1}\\n\")\n",
    "\n",
    "print(f\"Saved average scores to {file_path}\")"
   ],
   "id": "d1e0d374b29d74e0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
